ubuntu_tag: "noble-20250925"
amd_gpu_version: "6.4.4"
rocm_version: "6.4.4"
cassini_headers_version: "12.0.1"
cxi_headers_version: "12.0.1"
libcxi_version: "12.0.1"
libfabric_version: "1.22.0"
mpich_version: "4.3.2rc2"
aws_ofi_rccl_version: "1.4.0-1"
aws_ofi_rccl_commit: "6dae1b0"
torch_version: "2.8.0"
torch_url: "https://download.pytorch.org/whl/rocm6.4/torch-2.8.0%2Brocm6.4-cp312-cp312-manylinux_2_28_x86_64.whl"
torchaudio_url: "https://download.pytorch.org/whl/rocm6.4/torchaudio-2.8.0%2Brocm6.4-cp312-cp312-manylinux_2_28_x86_64.whl"
torchvision_url: "https://download.pytorch.org/whl/rocm6.4/torchvision-0.23.0%2Brocm6.4-cp312-cp312-manylinux_2_28_x86_64.whl"
pytorch_triton_url: "https://download.pytorch.org/whl/pytorch_triton_rocm-3.4.0-cp312-cp312-linux_x86_64.whl"
flash_attention_repo_commit: "465cb97"
flash_attention_repo_url: "https://github.com/ROCm/flash-attention"
flash_attention_gpu_archs: "gfx90a"
vllm_commit: "tags/v0.10.2"
vllm_gpu_archs: "gfx90a"
