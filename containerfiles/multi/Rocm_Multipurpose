
FROM $BASE_IMAGE:$BASE_IMAGE_TAG

ARG BASE_IMAGE
ARG BASE_IMAGE_TAG

ARG FLASH_ATTENTION_REPO_COMMIT
ARG FLASH_ATTENTION_REPO_URL
ARG FLASH_ATTENTION_GPU_ARCHS

ARG VLLM_COMMIT
ARG VLLM_GPU_ARCHS

ARG NPROC

RUN . /opt/venv/bin/activate \
        && pip3.12 install packaging ninja \
        && mkdir /opt/src \
        && git clone $FLASH_ATTENTION_REPO_URL /opt/src/flash-attention \
        && cd  /opt/src/flash-attention \
        && git checkout $FLASH_ATTENTION_REPO_COMMIT \
        && BUILD_TARGET=rocm MAX_JOBS=$NPROC GPU_ARCHS=$FLASH_ATTENTION_GPU_ARCHS python3 setup.py bdist_wheel --dist-dir=dist \
        && pip3.12 install /opt/src/flash-attention/dist/*.whl

RUN python3.12 -m venv /opt/venv \
	&& . /opt/venv/bin/activate \
        && apt-get update \
        && apt-get -y --no-install-recommends install cmake sqlite3 libsqlite3-dev libfmt-dev libmsgpack-dev libsuitesparse-dev apt-transport-https ca-certificates \
        && git clone https://github.com/vllm-project/vllm.git /opt/src/vllm \
        && cd /opt/rocm/share/amd_smi \
	&& python3 -m pip install . \
        && cd /opt/src/vllm \
	&& git fetch --tags \
	&& git checkout $VLLM_COMMIT \
	&& export GPU_ARCHS=$VLLM_GPU_ARCHS \
        && export PYTORCH_ROCM_ARCH=$VLLM_GPU_ARCHS \
        && export GPU_TARGETS=$VLLM_GPU_ARCHS \
        && export MAKEFLAGS="-j$NPROC" \
        && pip install setuptools_scm jinja2 \
	&& python3 setup.py clean --all \
	&& python3 setup.py bdist_wheel --dist-dir=dist \
	&& pip install ./dist/*.whl \
	&& apt-get -y install libglib2.0-dev ffmpeg libopencv-dev \
	&& pip install ultralytics \
	&& pip install mpi4py

ENV PATH="/opt/venv/bin:$PATH"

CMD ["/bin/bash"]
